{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:\n",
    " - Preprocess the text to remove the stopwords \n",
    " - Lemmatize pos tag each word\n",
    " - Select only noun and verb POS\n",
    " - Compare synsets of each pair of word and select only the overlaping synsets\n",
    " - List out all the overlaping synsets and compute their lemmas\n",
    " - Also compute frequency of each lemma from the documents\n",
    " - Now filter out top K lemmas based on the frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, line_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "\n",
    "# with open('../Data/stop_words.txt', 'r') as f:\n",
    "#     function_words = line_tokenize(f.read())\n",
    "\n",
    "\n",
    "function_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "def load_json(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: json.loads(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    db_path = '../data/DB.sqlite'\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    df_emails = pd.read_sql('SELECT * FROM Data', con=conn).drop('index', axis=1).reset_index(drop=True)\n",
    "    df_emails = load_json(df_emails)\n",
    "    \n",
    "    return df_emails\n",
    "\n",
    "\n",
    "def pos_tag_text(text):\n",
    "    return pos_tag(text)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    pattern_1 = re.compile(r'[^A-Za-z\\s]*')\n",
    "    text = pattern_1.sub('', text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = word_tokenize(text)\n",
    "\n",
    "    text = pos_tag_text(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def compute_overlap_score(synset, sentence):\n",
    "    gloss = set(word_tokenize(synset.definition()))\n",
    "\n",
    "    for i in synset.examples():\n",
    "        gloss.union(i)\n",
    "\n",
    "    gloss = gloss.difference(function_words)\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = set(sentence.split(\" \"))\n",
    "\n",
    "    elif isinstance(sentence, list):\n",
    "        sentence = set(sentence)\n",
    "\n",
    "    elif isinstance(sentence, set):\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    sentence = sentence.difference(function_words)\n",
    "\n",
    "    return len(gloss.intersection(sentence))\n",
    "\n",
    "\n",
    "def lesk(word, sentence):\n",
    "    best_sense = None\n",
    "    max_overlap = 0\n",
    "    word = wn.morphy(word) if wn.morphy(word) is not None else word\n",
    "\n",
    "    for sense in wn.synsets(word):\n",
    "        overlap = compute_overlap_score(sense, sentence)\n",
    "\n",
    "        for hyponym in sense.hyponyms():\n",
    "            overlap += compute_overlap_score(hyponym, sentence)\n",
    "\n",
    "        for hypernym in sense.hypernyms():\n",
    "            overlap += compute_overlap_score(hypernym, sentence)\n",
    "\n",
    "        for meronym in sense.part_meronyms():\n",
    "            overlap += compute_overlap_score(meronym, sentence)\n",
    "\n",
    "        for meronym in sense.substance_meronyms():\n",
    "            overlap += compute_overlap_score(meronym, sentence)\n",
    "\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = sense\n",
    "\n",
    "    return best_sense\n",
    "\n",
    "\n",
    "def get_all_senses(word):\n",
    "    return wn.synsets(word)\n",
    "\n",
    "\n",
    "def get_all_hypernyms(sense):\n",
    "    return sense.hypernyms()\n",
    "\n",
    "\n",
    "def merge_terms(pos_text):\n",
    "    text = []\n",
    "\n",
    "    for pos_word in pos_text:\n",
    "        if 'NN' in pos_word[1] or 'JJ' in pos_word[1] or 'VB' in pos_word[1]:\n",
    "            text.append(pos_word[0].lower())\n",
    "        \n",
    "    text = ' '.join(text)\n",
    "\n",
    "    doc_text = nlp(text)\n",
    "\n",
    "    text = [x.lemma_ for x in doc_text]\n",
    "\n",
    "    # --------- Using Lesk Algorithm to find best sense of every word ------------\n",
    "\n",
    "    word_sense_dict = {x: lesk(x, text) for x in text}\n",
    "\n",
    "    text = np.array(text)\n",
    "\n",
    "    # ------------- Merging terms with commons meanings --------------------------\n",
    "\n",
    "    for i in range(len(text)-1):\n",
    "        if word_sense_dict[text[i]] is not None:\n",
    "            for j in range(i+1, len(text)):\n",
    "                if text[i] != text[j]:\n",
    "                    if word_sense_dict[text[i]] in get_all_senses(text[j]):\n",
    "                        # print(f'Merged...{text[i]} and {text[j]}')\n",
    "                        text = np.where(text == text[j], text[i], text)\n",
    "\n",
    "    # ------------------- Merging terms with Hypernyms ---------------------------\n",
    "\n",
    "    for i in range(len(text)-1):\n",
    "        try:\n",
    "            if word_sense_dict[text[i]] is not None:\n",
    "                for j in range(i+1, len(text)):\n",
    "                    try:\n",
    "                        if (text[i] != text[j]) and (word_sense_dict[text[j]] is not None):\n",
    "                            word_sense_i = word_sense_dict[text[i]]\n",
    "                            word_sense_j = word_sense_dict[text[j]]\n",
    "\n",
    "                            hypernyms_i = get_all_hypernyms(word_sense_i)\n",
    "                            hypernyms_j = get_all_hypernyms(word_sense_j)\n",
    "\n",
    "                            if word_sense_i in hypernyms_j:\n",
    "                                # print(f'{text[i]} is a Hypernym of {text[j]}')\n",
    "                                text = np.where(text == text[j], text[i], text)\n",
    "\n",
    "                            elif word_sense_j in hypernyms_i:\n",
    "                                # print(f'{text[j]} is a Hypernym of {text[i]}')\n",
    "                                text = np.where(text == text[i], text[j], text)\n",
    "\n",
    "                            elif len(set(hypernyms_i).intersection(set(hypernyms_j)))>0:\n",
    "                                hypernym_lemma = set(hypernyms_i).intersection(set(hypernyms_j)\n",
    "                                                                               ).pop().lemmas()[0].name()\n",
    "                                \n",
    "                                hypernym_synset = set(hypernyms_i).intersection(set(hypernyms_j)).pop()\n",
    "\n",
    "                                # print(f'{text[i]} and {text[j]} have common hypernyms: {hypernym_lemma}')\n",
    "\n",
    "                                text = np.where((text == text[j]) | (text == text[i]), hypernym_lemma, text)\n",
    "                                \n",
    "                                word_sense_dict[hypernym_lemma] = hypernym_synset\n",
    "                                \n",
    "                    except KeyError as ke:\n",
    "                        continue\n",
    "\n",
    "        except KeyError as ke:\n",
    "            continue\n",
    "\n",
    "    return pd.Series({'preprocessed_text':' '.join(text), 'word_sense_dict':word_sense_dict})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_matrix(corpus):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "    count_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    count_feature_names = count_vectorizer.get_feature_names()\n",
    "    \n",
    "    df_matrix_tfidf = pd.DataFrame(tfidf_matrix.todense(), columns=tfidf_feature_names)\n",
    "    df_matrix_count = pd.DataFrame(count_matrix.todense(), columns=count_feature_names)\n",
    "\n",
    "    return df_matrix_tfidf, df_matrix_count, tfidf_vectorizer, count_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mail_body(x):\n",
    "    mail_body =  x['Mail_1']\n",
    "    \n",
    "    if 'Mail_2' in x.keys():\n",
    "        mail_body = mail_body + ' ' + x['Mail_2']\n",
    "        \n",
    "    pattern_1 = re.compile(r'[\\w\\.-_]+@[\\w\\.-_]+')\n",
    "    \n",
    "    text = pattern_1.sub('', mail_body)\n",
    "    \n",
    "    pattern_2 = re.compile(r'(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+')\n",
    "    \n",
    "    text = pattern_2.sub('', text)\n",
    "    \n",
    "    text = ' '.join(word_tokenize(text))\n",
    "        \n",
    "    pattern_3 = re.compile(r'[^A-Za-z\\s]*')\n",
    "    \n",
    "    text = pattern_3.sub('', text)\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_email = load_data()\n",
    "\n",
    "cats_to_consider = cats_to_consider = ['1_Class_Add_Invoice', '2_Class_Payment_Query']\n",
    "\n",
    "df_email = df_email.loc[df_email.CLASS.isin(cats_to_consider)]\n",
    "\n",
    "df_email.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_email = df_email.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Merging terms using Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_email['BODY'] = df_email.BODY.apply(preprocess_mail_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_email['text'] = df_email.SUBJECT + ' ' + df_email.BODY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_email['pos_text'] = df_email.text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_email = pd.concat([df_email, pd.DataFrame(df_email.pos_text.apply(merge_terms))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf, df_count, tfidf_vectorizer, count_vectorizer = build_tfidf_matrix(df_email.preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 19), (50, 210))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_email.shape, df_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature reduction using path similarity measure from wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(xi, xj, word_sense_dict):\n",
    "    word_sense_i = word_sense_dict[xi]\n",
    "    word_sense_j = word_sense_dict[xj]\n",
    "    \n",
    "    if (word_sense_i is None) or (word_sense_j is None):\n",
    "        return None\n",
    "    \n",
    "    return wn.path_similarity(word_sense_i, word_sense_j, simulate_root=False)\n",
    "\n",
    "\n",
    "def update_tfidf_scores(xi, xj, index, distance):\n",
    "    try:\n",
    "        tfidf_xi = df_tfidf.loc[index, xi]\n",
    "        tfidf_xj = df_tfidf.loc[index, xj]\n",
    "\n",
    "\n",
    "        tfidf_xi, tfidf_xj = np.array([tfidf_xi, tfidf_xj]) * (1 - distance)\n",
    "\n",
    "        df_tfidf.loc[index, xi] = tfidf_xi\n",
    "        df_tfidf.loc[index, xj] = tfidf_xj\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "scores_list = []\n",
    "\n",
    "\n",
    "def compute_path_based_similarity(x):\n",
    "    index = x['index']\n",
    "    text = x.preprocessed_text\n",
    "    x_tokens = word_tokenize(text)\n",
    "    word_sense_dict = x.word_sense_dict\n",
    "    threshold = 1\n",
    "    \n",
    "    dropped_tokens = []\n",
    "    \n",
    "    for i in range(len(x_tokens)-1):\n",
    "        token_is_related = False\n",
    "        \n",
    "        xi = x_tokens[i]\n",
    "        \n",
    "        for j in range(i+1, len(x_tokens)):\n",
    "            xj = x_tokens[j]\n",
    "            \n",
    "            if xi != xj:\n",
    "                distance = compute_distance(xi, xj, word_sense_dict)\n",
    "                                \n",
    "                if (distance is not None) and (distance <= threshold):\n",
    "                    token_is_related = True\n",
    "                    \n",
    "                    print(f'{xi} :: {xj} with score {distance}')\n",
    "                    \n",
    "                    update_tfidf_scores(xi, xj, index, distance)\n",
    "        \n",
    "        if not token_is_related:\n",
    "            pattern = re.compile(fr'\\b{xi}\\b')\n",
    "            \n",
    "            text = pattern.sub('', text)\n",
    "            \n",
    "            dropped_tokens.append(xi)\n",
    "            \n",
    "    print(f'Dropped the following words: {dropped_tokens}')\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_email.reset_index().apply(compute_path_based_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Chi-Square to compute best features for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count['Category'] = df_email.CLASS.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aamerican</th>\n",
       "      <th>aamva</th>\n",
       "      <th>ab</th>\n",
       "      <th>aba</th>\n",
       "      <th>abatement</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abm</th>\n",
       "      <th>above</th>\n",
       "      <th>...</th>\n",
       "      <th>zpos</th>\n",
       "      <th>zpr</th>\n",
       "      <th>zrp</th>\n",
       "      <th>zsn</th>\n",
       "      <th>zuora</th>\n",
       "      <th>zuras</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurora</th>\n",
       "      <th>zycus</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_Class_Add_Invoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_Class_Add_Invoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_Class_Add_Invoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_Class_Add_Invoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_Class_Add_Invoice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aamerican  aamva  ab  aba  abatement  abc  ability  able  abm  above  ...  \\\n",
       "0          0      0   0    0          0    0        0     0    0      0  ...   \n",
       "1          0      0   0    0          0    0        0     0    0      0  ...   \n",
       "2          0      0   0    0          0    0        0     0    0      0  ...   \n",
       "3          0      0   0    0          0    0        0     0    0      0  ...   \n",
       "4          0      0   0    0          0    0        0     0    0      0  ...   \n",
       "\n",
       "   zpos  zpr  zrp  zsn  zuora  zuras  zurich  zurora  zycus  \\\n",
       "0     0    0    0    0      0      0       0       0      0   \n",
       "1     0    0    0    0      0      0       0       0      0   \n",
       "2     0    0    0    0      0      0       0       0      0   \n",
       "3     0    0    0    0      0      0       0       0      0   \n",
       "4     0    0    0    0      0      0       0       0      0   \n",
       "\n",
       "              Category  \n",
       "0  1_Class_Add_Invoice  \n",
       "1  1_Class_Add_Invoice  \n",
       "2  1_Class_Add_Invoice  \n",
       "3  1_Class_Add_Invoice  \n",
       "4  1_Class_Add_Invoice  \n",
       "\n",
       "[5 rows x 3199 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_category = df_count.groupby('Category').sum().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df_term_category.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_j_dot = df_term_category.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_dot_k = df_term_category.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(a, b):\n",
    "    return 1 if a>=b else -1\n",
    "\n",
    "def compute_feature_contribution(njk, nj, nk, N):\n",
    "    fjk = njk/N\n",
    "    fjfk = nj*nk/N**2\n",
    "    \n",
    "    X2 = (fjk - fjfk)**2/(fjfk)*sign(fjk, fjfk)\n",
    "    \n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chi_sq = pd.DataFrame(index=df_term_category.index, columns=df_term_category.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in df_term_category.index:\n",
    "    for cat in df_term_category.columns:\n",
    "        df_chi_sq[cat][term] = compute_feature_contribution(df_term_category[cat][term], N_j_dot[term], N_dot_k[cat], N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>1_Class_Add_Invoice</th>\n",
       "      <th>2_Class_Payment_Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aamerican</th>\n",
       "      <td>4.12431e-08</td>\n",
       "      <td>-8.25901e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aamva</th>\n",
       "      <td>1.23729e-07</td>\n",
       "      <td>-2.4777e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>4.12431e-08</td>\n",
       "      <td>-8.25901e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aba</th>\n",
       "      <td>-1.30436e-06</td>\n",
       "      <td>2.612e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abatement</th>\n",
       "      <td>1.37477e-08</td>\n",
       "      <td>-2.753e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category  1_Class_Add_Invoice 2_Class_Payment_Query\n",
       "aamerican         4.12431e-08          -8.25901e-07\n",
       "aamva             1.23729e-07           -2.4777e-06\n",
       "ab                4.12431e-08          -8.25901e-07\n",
       "aba              -1.30436e-06             2.612e-05\n",
       "abatement         1.37477e-08            -2.753e-07"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chi_sq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Class_Add_Invoice : Index(['communication', 'intend', 'attachment', 'recipient', 'sender',\n",
      "       'delete', 'dissemination', 'privilege', 'reader', 'confidential'],\n",
      "      dtype='object')\n",
      "2_Class_Payment_Query : Index(['gregorian_calendar_month', 'give', 'status', 'update', 'due',\n",
      "       'payment', 'usd', 're', 'receive', 'get'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for cat in df_chi_sq.columns:\n",
    "    print(cat, ':', df_chi_sq[cat].sort_values(ascending=False)[:10].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_category = df_term_category.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the count matrix to tfidf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_category = pd.DataFrame(df_term_category.values/ df_term_category.sum(axis=1).values.reshape(-1, 1), columns=df_term_category.columns, \n",
    "                               index=df_term_category.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_category = pd.DataFrame(df_term_category.values * np.log(df_term_category.shape[0]/(df_term_category>0).sum()).values.reshape(1, -1), columns=df_term_category.columns,\n",
    "            index=df_term_category.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating category vector for top K categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>1_Class_Add_Invoice</th>\n",
       "      <th>2_Class_Payment_Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aamerican</th>\n",
       "      <td>4.12431e-08</td>\n",
       "      <td>-8.25901e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aamva</th>\n",
       "      <td>1.23729e-07</td>\n",
       "      <td>-2.4777e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>4.12431e-08</td>\n",
       "      <td>-8.25901e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aba</th>\n",
       "      <td>-1.30436e-06</td>\n",
       "      <td>2.612e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abatement</th>\n",
       "      <td>1.37477e-08</td>\n",
       "      <td>-2.753e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category  1_Class_Add_Invoice 2_Class_Payment_Query\n",
       "aamerican         4.12431e-08          -8.25901e-07\n",
       "aamva             1.23729e-07           -2.4777e-06\n",
       "ab                4.12431e-08          -8.25901e-07\n",
       "aba              -1.30436e-06             2.612e-05\n",
       "abatement         1.37477e-08            -2.753e-07"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chi_sq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weighted_vector_cat(df_term_cat, df_chi_sq, k):\n",
    "    cat_vec = {}\n",
    "    \n",
    "    for cat in df_term_cat.index:\n",
    "        top_k_cats = df_chi_sq[cat].sort_values(ascending=False)[:k].index.tolist()\n",
    "        cat_vec[cat] = (top_k_cats, df_term_cat.loc[cat, top_k_cats].values.tolist())\n",
    "    \n",
    "    return cat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vecs = generate_weighted_vector_cat(df_term_category, df_chi_sq, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class prediction using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(x, cat_vectors):\n",
    "    prediction = None\n",
    "    best_score = -1\n",
    "    \n",
    "    for cat in cat_vectors.keys():\n",
    "        top_terms = cat_vectors[cat][0]\n",
    "        cat_vec = np.array(cat_vectors[cat][1]).reshape(1, -1)\n",
    "        \n",
    "        x_cat = x[top_terms].values.reshape(1, -1)\n",
    "        \n",
    "        score = cosine_similarity(x_cat, cat_vec)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            prediction = cat\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_email['prediction'] = df_tfidf.apply(predict_category, axis=1, cat_vectors=cat_vecs).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_email.CLASS = le.fit_transform(df_email.CLASS)\n",
    "df_email.prediction = le.transform(df_email.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99428299, 0.83216783])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_email.CLASS, df_email.prediction, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9132254120343687"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df_email.CLASS, df_email.prediction, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.71257485])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(df_email.CLASS, df_email.prediction, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FILENAME', 'DATE', 'FROM', 'TO', 'CC', 'BCC', 'SUBJECT', 'BODY',\n",
       "       'GREETING', 'SIGNATURE', 'ATTACHMENT_FOUND', 'ATTACHMENTS_DETAILS',\n",
       "       'INVOICE_NO', 'CLASS', 'CUSTOMER', 'text', 'pos_text',\n",
       "       'preprocessed_text', 'prediction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_email.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_terms(df_term_cat, df_chi_sq, k):\n",
    "    top_cats = []\n",
    "    \n",
    "    for cat in df_term_cat.index:\n",
    "        top_cats = top_cats + df_chi_sq[cat].sort_values(ascending=False)[:k].index.tolist()\n",
    "    \n",
    "    return top_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_consider = top_k_terms(df_term_category, df_chi_sq, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_tfidf.loc[:, terms_to_consider], df_email.CLASS, test_size=0.2,\n",
    "                                                   stratify = df_email.CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier(n_estimators=100, max_depth=9, n_jobs=-1, random_state=123, class_weight='balanced')\n",
    "model = LogisticRegression(class_weight='balanced', multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='ovr', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9786975 , 0.63157895])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, preds, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
